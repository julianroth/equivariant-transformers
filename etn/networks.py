"""
Network architectures for classification and pose prediction in Transformer modules.
"""
from functools import partial
import tensorflow as tf
from tensorflow.keras import layers
import tensorflow_addons as tfa
import numpy as np
from .coordinates import identity_grid
from .transformers import projective_grid_transform


class TransformerCNN(tf.keras.Model):
    def __init__(self, net, transformer=None, coords=identity_grid, downsample=1,
                 grid_size_tf=None, ulim=None, vlim=None):
        """ A wrapper class that applies a sequence of transformers followed by a coordinate
        transformation before passing the result through a network.
    
        Args:
            net: tf.keras.Model, network to apply after transformers and coordinate transformation
            transformer: tf.keras.Model, transformer module returning a list with the
                         predicted transformation, and predicted parameters and heatmap from
                         the respective pose predictor module
            coords: Callable, coordinate transformation function
            downsample: int, input downsampling factor
            grid_size_tf: (int, int), grid size for the transformer module
            ulim: (float, float), limits for the u coordinate
            vlim: (float, float), limits for the v coordinate
        """
        super().__init__()
        self.transformer = transformer
        self.net = net
        self.coords = coords
        self.downsample = downsample
        self.grid_size_tf = grid_size_tf
        self.ulim = ulim
        self.vlim = vlim
    
    def call(self, inputs, training=None, mask=None):
        """
        Args:
            inputs:
            x: torch.Tensor, input tensor
            tf_output: bool, whether to return dict with intermediate values generated by the transformer(s)

        Returns:
            (torch.Tensor, dict) if tf_output else torch.tensor, output of the network
        """
        grid_size = (tf.shape(inputs)[-3] // self.downsample, tf.shape(inputs)[-2] // self.downsample)
        grid_size_tf = self.grid_size_tf
        if grid_size_tf is None:
            grid_size_tf = tf.shape(inputs)[-3:-1]

        # input sampling grid: we start off with the grid representing the final coordinate transformation.
        # we then transform the grid using the transformation predicted by the transformer module.
        # you can convince yourself that resampling an image with a grid transformed by T_theta is equivalent
        # to first applying the inverse transformation T^{-1}_theta to the image, followed by the coordinate
        # transformation.
        if self.ulim is not None and self.vlim is not None:
            grid = self.coords(grid_size, ulim=self.ulim, vlim=self.vlim)
        else:
            grid = self.coords(grid_size)
        grid = tf.tile(tf.expand_dims(grid, axis=0), [tf.shape(inputs)[0], 1, 1, 1])

        # predict the transformation from the input image and apply the predicted transformation to the grid
        if self.transformer is not None:
            tf_out = self.transformer([inputs, grid_size_tf, None], training=training)
            transform = tf_out['transform']
            if type(transform) is list:
                transform = transform[-1]
            grid = projective_grid_transform(transform, grid)
        else:
            tf_out = None

        # transform the input image
        grid = (grid + 1.) * (tf.reshape(tf.cast(grid_size, dtype=tf.float32), [1, 1, 1, 2]) / 2.)
        out = tfa.image.resampler(inputs, grid)

        # pass the transformed image to the rest of the network
        out = self.net(out, training=training)

        return out, tf_out
            


# =================================================================================
# Helper functions that allow for cyclic (i.e., wrap-around) padding along an axis.
# We use cyclic padding when applying the CNN over coordinate systems that are
# periodic in at least one dimension (e.g., polar and log-polar coordinates).
# =================================================================================


def _pad1d(x, pad, mode):
    """1D padding.

    Args:
        x: tf.Tensor, input tensor
        pad: int, pad amount
        mode: str, one of 'constant', 'reflect', 'symmetric', or 'cyclic'

    Returns:
        tf.Tensor, padded tensor
    """
    out = x
    if mode == 'cyclic':
        out = _cyclic_pad(x, pad=pad, axis=1)
    elif mode is not None:
        out = tf.pad(x, [[0, 0], [pad, pad], [0, 0]], mode)
    return out


def _pad2d(x, pad, mode):
    """2D padding.

    Args:
        x: tf.Tensor, input tensor
        pad: int or (int, int), pad amount
        mode: str or (str, str), one of 'constant', 'reflect', 'symmetric', or 'cyclic'

    Returns:
        tf.Tensor, padded tensor
    """
    if type(pad) is int:
        pad = (pad, pad)
    if type(mode) is str:
        mode = (mode, mode)
    wmode, hmode = mode
    wpad, hpad = pad
    out = x
    if wmode == 'cyclic':
        out = _cyclic_pad(out, pad=wpad, axis=2)
    elif wmode is not None:
        out = tf.pad(out, [[0, 0], [0, 0], [wpad, wpad], [0, 0]], wmode)
    if hmode == 'cyclic':
        out = _cyclic_pad(out, pad=hpad, axis=1)
    elif hmode is not None:
        out = tf.pad(out, [[0, 0], [hpad, hpad], [0, 0], [0, 0]], hmode)
    return out


def _cyclic_pad(x, pad, axis):
    """Cyclic padding.

    Args:
        x: torch.Tensor, input tensor
        pad: int or (int, int), pad amount
        axis: int, axis along which to pad

    Returns:
        torch.Tensor, padded tensor
    """
    if type(pad) is int:
        pad = (pad, pad)
    if pad[0] == 0 and pad[1] == 0:
        return x
    rank = tf.rank(x)
    if pad[1] > 0:
        begin = tf.zeros([rank], dtype=tf.int32)
        size = tf.concat([
            tf.shape(x)[:axis],
            pad[1:],
            tf.shape(x)[axis + 1:]
        ], axis=0)
        left = tf.slice(x, begin, size)
    if pad[0] > 0:
        begin = tf.concat([
            tf.zeros([axis], dtype=tf.int32),
            tf.shape(x)[axis:axis + 1] - pad[0:1],
            tf.zeros([rank - (axis + 1)], dtype=tf.int32)
        ], axis=0)
        size = tf.concat([
            tf.shape(x)[:axis],
            pad[0:1],
            tf.shape(x)[axis + 1:]
        ], axis=0)
        right = tf.slice(x, begin, size)
    if pad[0] == 0:
        return tf.concat([x, left], axis)
    if pad[1] == 0:
        return tf.concat([right, x], axis)
    return tf.concat([right, x, left], axis)


# =================================================================================
# CNN architectures
# =================================================================================

def make_basic_cnn(input_shape,
                   output_size=10,
                   nf=20,
                   p_dropout=0.,
                   pad_mode=('constant', 'constant'),
                   strides=(1, 1, 1),
                   pool=(True, True, False)):
    """Creates and returns a CNN architecture based on the 7-layer Z2CNN network from Cohen & Welling (2016).

    Args:
        input_shape: (int, int, int), shape of the input (H, W, C)
        output_size: int, output dimension
        nf: int, number of channels in each convolutional layer
        p_dropout: float, dropout probability after the 3rd and 6th convolutional layers
        pad_mode: (string, string), padding mode for convolutional layers,
                  one of {'constant', 'reflect', 'symmetric', 'cyclic'}
        strides: (int, int, int), strides of first three convolutional layers
        pool: (bool, bool, bool), whether to average pool after the first three convolutional layers
    """
    model = tf.keras.Sequential()
    model.add(layers.InputLayer(input_shape, dtype=tf.float32))
    for i in range(3):
        model.add(layers.Lambda(partial(_pad2d, pad=1, mode=pad_mode)))
        model.add(layers.Conv2D(nf, kernel_size=3, strides=strides[i], use_bias=False))
        model.add(layers.BatchNormalization())
        model.add(layers.ReLU())
        
        if pool[i]:
            model.add(layers.AveragePooling2D(pool_size=2, strides=2))
    model.add(layers.Dropout(p_dropout))
    
    for i in range(3):
        model.add(layers.Lambda(partial(_pad2d, pad=1, mode=pad_mode), input_shape=input_shape))
        model.add(layers.Conv2D(nf, kernel_size=3, strides=strides[2], use_bias=False))
        model.add(layers.BatchNormalization())
        model.add(layers.ReLU())
    
    model.add(layers.Dropout(p_dropout))
    
    model.add(layers.Lambda(partial(_pad2d, pad=1, mode=pad_mode), input_shape=input_shape))
    model.add(layers.Conv2D(output_size, kernel_size=3, strides=strides[2], use_bias=False))
    
    model.add(layers.Reshape([output_size, -1]))
    model.add(layers.Lambda(partial(tf.math.reduce_max, axis=-1)))
    return model


'''
# =================================================================================
# ResNet architectures
# Adapted from //github.com/pytorch/vision/blob/master/torchvision/models/resnet.py
# =================================================================================

class BasicBlock(layers.Layer):
    def __init__(self, planes, strides=1, downsample=None, p_dropout=0.,
                 nonlin=tf.nn.relu, padding_mode='constant'):
        super().__init__()
        self.nonlin = nonlin
        self.padding_mode = padding_mode
        
        self.conv1 = layers.Conv2D(planes, kernel_size=3, strides=strides, use_bias=False)
        self.bn1 = layers.BatchNormalization()
        self.dropout = layers.Dropout(p_dropout)
        self.conv2 = layers.Conv2D(planes, kernel_size=3, strides=strides, use_bias=False)
        self.bn2 = layers.BatchNormalization()
        self.downsample = downsample
    
    def call(self, inputs, **kwargs):
        residual = inputs
        out = _pad2d(inputs, 1, self.padding_mode)
        out = self.conv1(out, **kwargs)
        out = self.bn1(out, **kwargs)
        out = self.nonlin(out, **kwargs)
        out = self.dropout(out, **kwargs)
        out = _pad2d(inputs, 1, self.padding_mode)
        out = self.conv2(out, **kwargs)
        out = self.bn2(out, **kwargs)
        
        if self.downsample is not None:
            residual = self.downsample(inputs)
        
        out = out + residual
        out = self.nonlin(out)
        return out


def make_resnet(block, num_layers,
                 output_size=10,
                 output_bias=True,
                 nf=32,
                 in_channels=3,
                 in_kernel_size=5,
                 in_stride=1,
                 in_padding=0,
                 nonlin=tf.nn.relu,
                 p_dropout=0.,
                 pad_mode=('constant', 'constant')):
    model = tf.keras.Sequential()
    model.add(layers.Lambda(partial(_pad2d, 2, pad_mode)))
    model.add(layers.Conv2D(nf, kernel_size=in_kernel_size, strides=in_stride, padding=in_padding, use_bias=False))
        
'''


# =========================================================
# Pose prediction modules
# =========================================================

class Bias(layers.Layer):
    def __init__(self, activation=tf.nn.tanh):
        super().__init__()
        self.activation = activation
    
    def build(self, input_shape):
        if len(input_shape) == 1:
            input_shape = (None, 1)
        self.b = self.add_weight(shape=(input_shape[-1],),
                                 dtype=tf.float32,
                                 initializer='zeros',
                                 trainable=True)
    
    def call(self, inputs, **kwargs):
        return inputs + self.activation(self.b)


def _centroid(heatmap, step, periodic=False):
    """
    Computes the centroid for each vector in a batch.

    If `periodic` is set to true, computes the centroid of points on the unit disk and then
    returns the scaled angle of the centroid.

    Args:
        heatmap: tf.Tensor, 2D tensor where the first dimension is the batch dimension
        step: float, interval between consecutive entries in tensor
        periodic: bool, whether the axis over which the centroid is to be computed is cyclic

    Returns:
        tf.Tensor, 1D tensor of centroids
    """
    rnge = tf.range(tf.shape(heatmap)[1], dtype=tf.float32) * step
    rnge = rnge - rnge[-1] / 2.  # center at 0
    
    if periodic:
        thetas = rnge * np.pi
        xs = tf.cos(thetas)
        ys = tf.sin(thetas)
        x_c = tf.linalg.matvec(heatmap, xs)
        y_c = tf.linalg.matvec(heatmap, ys)
        return tf.atan2(y_c, x_c) / np.pi
    else:
        return tf.linalg.matvec(heatmap, rnge)


def make_equivariant_pose_predictor(input_shape,
                                    nf,
                                    kernel_size=5,
                                    strides=(2, 2),
                                    periodic_u=False,
                                    periodic_v=False,
                                    return_u=True,
                                    return_v=True,
                                    nonlin=lambda x: tf.nn.leaky_relu(x, 0.1),
                                    **kwargs):
    """
    A translation-equivariant CNN for predicting transformation parameters.

    Args:
        input_shape: list, shape of input
        nf: int, number of channels in convolutional layers
        kernel_size: int, kernel width in convolutional layers
        strides: (int, int), convolutional layer strides
        periodic_u: bool, whether the u coordinate is periodic
        periodic_v: bool, whether the v coordinate is periodic
        return_u: bool, whether to return the predicted u coordinate
        return_v: bool, whether to return the predicted v coordinate
        nonlin: Callable, nonlinearity
    """
    delta = 2. * strides[0] * strides[1]
    vdelta, udelta = delta / (input_shape[0] - 1), delta / (input_shape[1] - 1)
    wmode = 'cyclic' if periodic_u else 'constant'
    hmode = 'cyclic' if periodic_v else 'constant'
    padding_mode = wmode, hmode
    inputs = layers.Input(shape=input_shape)
    
    out = _pad2d(inputs, kernel_size // 2, padding_mode)
    out = layers.Conv2D(nf, kernel_size, strides[0], use_bias=False)(out)
    out = layers.BatchNormalization()(out)
    out = nonlin(out)
    
    out = _pad2d(out, kernel_size // 2, padding_mode)
    out = layers.Conv2D(nf, kernel_size, strides[1], use_bias=False)(out)
    out = layers.BatchNormalization()(out)
    phi = nonlin(out)
    
    if return_u:
        out_u = tf.math.reduce_max(phi, axis=1)
        out_u = _pad1d(out_u, kernel_size // 2, wmode)
        out_u = layers.Conv1D(1, kernel_size, strides=1, use_bias=False)(out_u)
        out_u = tf.squeeze(out_u, axis=-1)
        heatmap_u = tf.nn.softmax(out_u, axis=-1)
        u = _centroid(heatmap_u, udelta, periodic_u)
        u = Bias()(u)
        if not return_v:
            outputs = (u, heatmap_u)
            model = tf.keras.Model(inputs=inputs, outputs=outputs)
            return model
    
    if return_v:
        out_v = tf.math.reduce_max(phi, axis=2)
        out_v = _pad1d(out_v, kernel_size // 2, hmode)
        out_v = layers.Conv1D(1, kernel_size, strides=1, use_bias=False)(out_v)
        out_v = tf.squeeze(out_v, axis=-1)
        heatmap_v = tf.nn.softmax(out_v, axis=-1)
        v = _centroid(heatmap_v, vdelta, periodic_v)
        v = Bias()(v)
        if not return_u:
            outputs = (v, heatmap_v)
            model = tf.keras.Model(inputs=inputs, outputs=outputs)
            return model
    
    outputs = ((u, v), (heatmap_u, heatmap_v))
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model


def make_direct_pose_predictor(input_shape,
                               nf,
                               kernel_size=5,
                               strides=(2, 2),
                               periodic_u=False,
                               periodic_v=False,
                               nonlin=lambda x: tf.nn.leaky_relu(x, 0.1),
                               num_outputs=1,
                               f_output='tanh',
                               **kwargs):
    """
    A CNN that predicts transformation parameters "directly", i.e. in a non-equivariant manner.

    Args:
        in_channels: int, number of input channels
        nf: int, number of filters in convolutional layers
        kernel_size: int, kernel width in convolutional layers
        strides: (int, int), convolutional layer strides
        periodic_u: bool, whether the u coordinate is periodic
        periodic_v: bool, whether the v coordinate is periodic
        nonlin: Callable, nonlinearity between convolutional layers
        num_outputs: int, number of parameters to predict
        f_output: str or Callable, nonlinearity to apply to the output
    """
    wmode = 'cyclic' if periodic_u else 'constant'
    hmode = 'cyclic' if periodic_v else 'constant'
    padding_mode = wmode, hmode
    
    inputs = layers.Input(shape=input_shape)

    out = _pad2d(inputs, kernel_size // 2, padding_mode)
    out = layers.Conv2D(nf, kernel_size, strides[0], use_bias=False)(out)
    out = layers.BatchNormalization()(out)
    out = nonlin(out)

    out = _pad2d(out, kernel_size // 2, padding_mode)
    out = layers.Conv2D(nf, kernel_size, strides[1], use_bias=False)(out)
    out = layers.BatchNormalization()(out)
    out = nonlin(out)
    
    out = layers.Flatten()(out)
    out = layers.Dense(num_outputs, activation=f_output)(out)
    
    outputs = out, None
    model = tf.keras.Model(inputs=inputs, outputs=outputs)
    return model
